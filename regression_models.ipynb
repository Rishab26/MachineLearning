{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The training and test R2 for the Lasso model with default parameters on the scikit-learn version of diabetes and the number of features used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetic_Data = sklearn.datasets.load_diabetes(return_X_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetic_Data['data'],diabetic_Data['target'], train_size = 0.7,  test_size = 0.3, random_state =1194)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_1 = sklearn.linear_model.Lasso()\n",
    "lasso_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training rsq is\n",
      "0.355922215834\n",
      " The testing rsq is\n",
      "0.325760438018\n"
     ]
    }
   ],
   "source": [
    "training_rsq1 = lasso_1.score(X_train,y_train)\n",
    "print(\"The training rsq is\")\n",
    "print(training_rsq1)\n",
    "test1_rsq1 = lasso_1.score(X_test,y_test)\n",
    "print(\" The testing rsq is\")\n",
    "print(test1_rsq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient values\n",
      "[   0.           -0.          385.75068422    0.            0.            0.\n",
      "   -0.            0.          275.39667358    0.        ]\n",
      "Count of non zero features used by the model\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bmi', 's5']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Coefficient values\")\n",
    "print(lasso_1.coef_)\n",
    "t =(lasso_1.coef_!=0)\n",
    "nz_coef = np.sum(lasso_1.coef_!=0)\n",
    "print(\"Count of non zero features used by the model\")\n",
    "print(nz_coef)\n",
    "#diabetic_Data.feature_names[(indices(lasso_1.coef == True).astype(int))]\n",
    "list(compress((diabetic_Data['feature_names']), t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The training and test R2 for the Lasso model with default parameters on the original version of diabetes and the number of features used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4,5,6 \n",
    "## Note : Data was taken from local , file uploaded with assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rsq is\n",
      "0.513336904352\n",
      "testing rsq is\n",
      "0.480696748963\n",
      "Coefficient values\n",
      "[  0.18265278 -18.50977488   6.02196645   1.03828218   0.33538326\n",
      "  -0.48876182  -1.41605417   1.32468838  22.8469203    0.37957936]\n",
      "Count of non zero features used by the model\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_data_original = pd.read_csv(\"D:\\\\Downloads\\\\diabetes.data\" , delimiter = '\\t')\n",
    "X_train, X_test, y_train, y_test = train_test_split(db_data_original.loc[:, 'AGE':'S6'],db_data_original['Y'], train_size = 0.7,  test_size = 0.3, random_state =1194)\n",
    "lasso_2_ns = sklearn.linear_model.Lasso()\n",
    "lasso_2_ns.fit(X_train,y_train)\n",
    "lasso_2_ns.get_params()\n",
    "training_rsq2_ns = lasso_2_ns.score(X_train,y_train)\n",
    "print(\"training rsq is\")\n",
    "print(training_rsq2_ns)\n",
    "test1_rsq2_ns = lasso_2_ns.score(X_test,y_test)\n",
    "print(\"testing rsq is\")\n",
    "print(test1_rsq2_ns)\n",
    "print(\"Coefficient values\")\n",
    "print(lasso_2_ns.coef_)\n",
    "t =(lasso_2_ns.coef_!=0)\n",
    "nz_coef = np.sum(lasso_2_ns.coef_!=0)\n",
    "print(\"Count of non zero features used by the model\")\n",
    "print(nz_coef)\n",
    "list(compress((diabetic_Data['feature_names']), t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments :\n",
    "#### Number of features used by the model is higher when using original dataset.(Non zero features)\n",
    "#### Training r_sq and testing r_sq is also higher for the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rsq is\n",
      "0.516404857178\n",
      "testing rsq is\n",
      "0.473797484443\n",
      "Coefficient values\n",
      "[  1.25891188  -9.72042598  25.18519869  13.53672564  -4.45066467  -0.\n",
      " -11.57330687   1.26991615  20.85481224   3.2474242 ]\n",
      "Count of non zero features used by the model\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(db_data_original.loc[:, 'AGE':'S6'],db_data_original['Y'], train_size = 0.7,  test_size = 0.3, random_state =1194)\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.fit_transform(X_test)\n",
    "lasso_2 = sklearn.linear_model.Lasso()\n",
    "lasso_2.fit(X_train_scale,y_train)\n",
    "lasso_2.get_params()\n",
    "training_rsq2 = lasso_2.score(X_train_scale,y_train)\n",
    "print(\"training rsq is\")\n",
    "print(training_rsq2)\n",
    "test2_rsq2 = lasso_2.score(X_test_scale,y_test)\n",
    "print(\"testing rsq is\")\n",
    "print(test2_rsq2)\n",
    "print(\"Coefficient values\")\n",
    "print(lasso_2.coef_)\n",
    "t_scaled =(lasso_2.coef_!=0)\n",
    "nz_coef_scaled = np.sum(lasso_2.coef_!=0)\n",
    "print(\"Count of non zero features used by the model\")\n",
    "print(nz_coef_scaled)\n",
    "#diabetic_Data.feature_names[(indices(lasso_1.coef == True).astype(int))]\n",
    "list(compress((diabetic_Data['feature_names']), t_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments :\n",
    "#### training rq and testing rsq results are closer to results in item 6 and not item 3 , Number of non zero features = 9 (very close to item 6) ,Hence penalizing is very similar to item 6\n",
    "#### Features are normalized but the Y is not \n",
    "#### Features are normalized differently between original and scikitlearn datasets \n",
    "\n",
    "#### Standard scaler uses z = (x - u) / s whereas the original data has been scaled differently. Thus dependinig on the input values the coefficients and impact of coefficients will be different which also results in a different alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "alphas = np.arange(0.01,20,0.01)\n",
    "no_of_parameters =[]\n",
    "test_rsq_alpha = []\n",
    "for alpha in alphas:\n",
    "    lasso_diffalpha = sklearn.linear_model.Lasso(alpha = alpha)\n",
    "    #print(alpha)\n",
    "    lasso_diffalpha.fit(X_train_scale,y_train)\n",
    "    training_rsq2_diffalpha = lasso_diffalpha.score(X_train_scale,y_train)\n",
    "    #print(training_rsq2_diffalpha)\n",
    "    test_rsq_alpha_toappend = lasso_diffalpha.score(X_test_scale,y_test)\n",
    "    #print(test_rsq_alpha_toappend)\n",
    "    test_rsq_alpha.append(test_rsq_alpha_toappend)\n",
    "    #print(np.sum(lasso_diffalpha.coef_!=0))\n",
    "    no_of_parameters.append(np.sum(lasso_diffalpha.coef_!=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Test R_Sq')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntSRhRERELf8fJdNxov2AjX8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c5fa47748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(no_of_parameters,test_rsq_alpha)\n",
    "plt.xlabel(\"Number of parameters\")\n",
    "plt.ylabel(\"Test R_Sq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments :\n",
    "#### Y axis is test r_sq - I prefer Number of parameters = 8 ot 9 and test_rsq = 0.47...  since it has the better r_sq value(measure of fit) when compare to the other points. We notice that \n",
    "#### 1) After a certain alpha value , test r_sq decreases while fitting  9 to 10 parameters. \n",
    "#### 2)higher the alpha , lower parameters it fits , leading to a lower test rsq. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter alpha\n",
      "0.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso_findreg= sklearn.linear_model.Lasso(max_iter=5000 , random_state =1194)\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "find_reg = GridSearchCV(lasso_findreg, tuned_parameters, cv=10)\n",
    "find_reg.fit(X_train_scale,y_train)\n",
    "print(\"Regularization parameter alpha\")\n",
    "print(find_reg.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training s_sq is\n",
      "0.518745148666\n",
      "Test R_Sq is\n",
      "0.474267279142\n",
      "Alpha value\n",
      "0.27\n",
      "Coef values\n",
      "[  1.95620987 -11.14669232  25.42122104  14.22124087  -9.09919206   0.\n",
      "  -8.13594849   6.58240594  21.46020592   3.72295986]\n",
      "Number of parameters used\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "reg = LassoCV(cv=10,alphas = alphas, random_state=1194).fit(X_train_scale, y_train)\n",
    "tr_r2 = reg.score(X_train_scale, y_train)\n",
    "print(\"Training s_sq is\")\n",
    "print(tr_r2)\n",
    "test_r2 = reg.score(X_test_scale,y_test)\n",
    "print(\"Test R_Sq is\")\n",
    "print(test_r2)\n",
    "print(\"Alpha value\")\n",
    "print(reg.alpha_)\n",
    "print(\"Coef values\")\n",
    "print(reg.coef_)\n",
    "print(\"Number of parameters used\")\n",
    "print(np.sum(reg.coef_!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Implementation of inductive conformal predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scale_tosplit = pd.DataFrame(X_train_scale)\n",
    "X_test_scale_tosplit = pd.DataFrame(X_test_scale)\n",
    "X_train_calib = pd.DataFrame()\n",
    "y_train_calib =pd.DataFrame()\n",
    "X_train_trainingproper, X_train_calib,y_trainingproper,y_train_calib= train_test_split(X_train_scale_tosplit,y_train ,train_size = 0.68, random_state =1294)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_trainingproper)\n",
    "scaled_X_train_trainingproper = scaler.transform(X_train_trainingproper)\n",
    "scaled_X_train_calib = scaler.transform(X_train_calib)\n",
    "scaled_X_test = scaler.transform(X_test_scale_tosplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the Lasso to the training set proper.\n",
    "y_calib_pred = lasso_ip.predict(scaled_X_train_calib)\n",
    "y_calib_pred= pd.DataFrame(y_calib_pred)\n",
    "y_test_pred_2 = lasso_ip.predict(scaled_X_test)\n",
    "y_test_pred_2= pd.DataFrame(y_test_pred_2)\n",
    "alpha_i = abs(y_train_calib - y_calib_pred)\n",
    "#Compute the alphai for the calibration set by the formula alpha = y(truelabel) - y(predicted by lasso)\n",
    "#being the true label and ^yi being the Lasso prediction)\n",
    "sorted_alpha = sorted(alpha_i)\n",
    "y_train_calib =pd.DataFrame(y_train_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "e = 0.05\n",
    "e_2 = 0.2\n",
    "nrow = np.shape(sorted_alpha)\n",
    "m = nrow[0]\n",
    "k =np.float_([(1-e)*(m+1)])\n",
    "k2 = np.float_([(1-e_2)*(m+1)])\n",
    "k = int(k)\n",
    "k2 = int(k2)\n",
    "c= sorted_alpha[k]\n",
    "c2 =sorted_alpha[k2]\n",
    "y_test_pred_2['lower_lim_set_sig_5%'] = pd.Series( y_test_pred_2[0] - c, index=y_calib_pred.index)\n",
    "y_test_pred_2['upper_lim_set_sig_5%'] = pd.Series( y_test_pred_2[0] + c, index=y_calib_pred.index)\n",
    "y_test_pred_2['lower_lim_set_sig_20%'] = pd.Series( y_test_pred_2[0] - c2, index=y_calib_pred.index)\n",
    "y_test_pred_2['upper_lim_set_sig_20%'] = pd.Series( y_test_pred_2[0] + c2, index=y_calib_pred.index)\n",
    "y_train_calib.reset_index(inplace=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =pd.DataFrame(y_test)\n",
    "y_test_pred_2['true_label'] = pd.Series(y_test['Y'], index = y_calib_pred.index )\n",
    "y_calib_pred['test_score_sig_5%'] = np.where((y_test_pred_2['lower_lim_set_sig_5%']<= y_test_pred_2['true_label']) & (y_test_pred_2['upper_lim_set_sig_5%']>= y_test_pred_2['true_label']), 1, 0)\n",
    "y_calib_pred['test_score_sig_20%'] = np.where((y_test_pred_2['lower_lim_set_sig_20%']<= y_test_pred_2['true_label']) & (y_test_pred_2['upper_lim_set_sig_20%']>= y_test_pred_2['true_label']), 1, 0)\n",
    "\n",
    "count_0_sig5 = (y_test_pred_2['test_score_sig_5%'] == 0).sum()\n",
    "count_0_sig20 = (y_test_pred_2['test_score_sig_20%'] == 0).sum()\n",
    "test_error_conf_pred_sig5 = count_0_sig5/m\n",
    "test_error_conf_pred_sig20 = count_0_sig20/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The lengths of prediction intervals and their test error rates at signifcance levels 5% and 20% (if you are implementing an inductive conformal predictor).\n",
    "\n",
    "### Note :All prediction intervals for test samples is present in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>lower_lim_set_sig_5%</th>\n",
       "      <th>upper_lim_set_sig_5%</th>\n",
       "      <th>lower_lim_set_sig_20%</th>\n",
       "      <th>upper_lim_set_sig_20%</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.857993</td>\n",
       "      <td>-282.142007</td>\n",
       "      <td>565.857993</td>\n",
       "      <td>-226.142007</td>\n",
       "      <td>509.857993</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.794818</td>\n",
       "      <td>-263.205182</td>\n",
       "      <td>584.794818</td>\n",
       "      <td>-207.205182</td>\n",
       "      <td>528.794818</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160.026839</td>\n",
       "      <td>-263.973161</td>\n",
       "      <td>584.026839</td>\n",
       "      <td>-207.973161</td>\n",
       "      <td>528.026839</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.380423</td>\n",
       "      <td>-239.619577</td>\n",
       "      <td>608.380423</td>\n",
       "      <td>-183.619577</td>\n",
       "      <td>552.380423</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.362692</td>\n",
       "      <td>-276.637308</td>\n",
       "      <td>571.362692</td>\n",
       "      <td>-220.637308</td>\n",
       "      <td>515.362692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154.236193</td>\n",
       "      <td>-269.763807</td>\n",
       "      <td>578.236193</td>\n",
       "      <td>-213.763807</td>\n",
       "      <td>522.236193</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>149.297739</td>\n",
       "      <td>-274.702261</td>\n",
       "      <td>573.297739</td>\n",
       "      <td>-218.702261</td>\n",
       "      <td>517.297739</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>182.900183</td>\n",
       "      <td>-241.099817</td>\n",
       "      <td>606.900183</td>\n",
       "      <td>-185.099817</td>\n",
       "      <td>550.900183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>165.991866</td>\n",
       "      <td>-258.008134</td>\n",
       "      <td>589.991866</td>\n",
       "      <td>-202.008134</td>\n",
       "      <td>533.991866</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119.126067</td>\n",
       "      <td>-304.873933</td>\n",
       "      <td>543.126067</td>\n",
       "      <td>-248.873933</td>\n",
       "      <td>487.126067</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>167.888482</td>\n",
       "      <td>-256.111518</td>\n",
       "      <td>591.888482</td>\n",
       "      <td>-200.111518</td>\n",
       "      <td>535.888482</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>108.451820</td>\n",
       "      <td>-315.548180</td>\n",
       "      <td>532.451820</td>\n",
       "      <td>-259.548180</td>\n",
       "      <td>476.451820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>107.227924</td>\n",
       "      <td>-316.772076</td>\n",
       "      <td>531.227924</td>\n",
       "      <td>-260.772076</td>\n",
       "      <td>475.227924</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>219.955514</td>\n",
       "      <td>-204.044486</td>\n",
       "      <td>643.955514</td>\n",
       "      <td>-148.044486</td>\n",
       "      <td>587.955514</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>140.571496</td>\n",
       "      <td>-283.428504</td>\n",
       "      <td>564.571496</td>\n",
       "      <td>-227.428504</td>\n",
       "      <td>508.571496</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>197.189960</td>\n",
       "      <td>-226.810040</td>\n",
       "      <td>621.189960</td>\n",
       "      <td>-170.810040</td>\n",
       "      <td>565.189960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>82.459703</td>\n",
       "      <td>-341.540297</td>\n",
       "      <td>506.459703</td>\n",
       "      <td>-285.540297</td>\n",
       "      <td>450.459703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200.316383</td>\n",
       "      <td>-223.683617</td>\n",
       "      <td>624.316383</td>\n",
       "      <td>-167.683617</td>\n",
       "      <td>568.316383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>173.343209</td>\n",
       "      <td>-250.656791</td>\n",
       "      <td>597.343209</td>\n",
       "      <td>-194.656791</td>\n",
       "      <td>541.343209</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>157.294197</td>\n",
       "      <td>-266.705803</td>\n",
       "      <td>581.294197</td>\n",
       "      <td>-210.705803</td>\n",
       "      <td>525.294197</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>61.785655</td>\n",
       "      <td>-362.214345</td>\n",
       "      <td>485.785655</td>\n",
       "      <td>-306.214345</td>\n",
       "      <td>429.785655</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>85.382582</td>\n",
       "      <td>-338.617418</td>\n",
       "      <td>509.382582</td>\n",
       "      <td>-282.617418</td>\n",
       "      <td>453.382582</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>72.339728</td>\n",
       "      <td>-351.660272</td>\n",
       "      <td>496.339728</td>\n",
       "      <td>-295.660272</td>\n",
       "      <td>440.339728</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>177.277197</td>\n",
       "      <td>-246.722803</td>\n",
       "      <td>601.277197</td>\n",
       "      <td>-190.722803</td>\n",
       "      <td>545.277197</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>106.096554</td>\n",
       "      <td>-317.903446</td>\n",
       "      <td>530.096554</td>\n",
       "      <td>-261.903446</td>\n",
       "      <td>474.096554</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>168.356287</td>\n",
       "      <td>-255.643713</td>\n",
       "      <td>592.356287</td>\n",
       "      <td>-199.643713</td>\n",
       "      <td>536.356287</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>101.896663</td>\n",
       "      <td>-322.103337</td>\n",
       "      <td>525.896663</td>\n",
       "      <td>-266.103337</td>\n",
       "      <td>469.896663</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>171.587117</td>\n",
       "      <td>-252.412883</td>\n",
       "      <td>595.587117</td>\n",
       "      <td>-196.412883</td>\n",
       "      <td>539.587117</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.268041</td>\n",
       "      <td>-373.731959</td>\n",
       "      <td>474.268041</td>\n",
       "      <td>-317.731959</td>\n",
       "      <td>418.268041</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>207.381926</td>\n",
       "      <td>-216.618074</td>\n",
       "      <td>631.381926</td>\n",
       "      <td>-160.618074</td>\n",
       "      <td>575.381926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>228.246909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>71.429132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>90.356813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>96.652668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>108.689151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>173.233004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>251.276862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>151.998726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>125.932603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>186.182243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>245.275804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>190.501091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>253.293191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>139.774058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>190.591313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>233.721829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>123.721149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>163.152534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>125.887355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>98.870407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>265.090764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>139.216375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>119.234854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>97.807567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>126.773098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>142.244951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>177.125739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>200.285030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>134.966040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>48.679824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0  lower_lim_set_sig_5%  upper_lim_set_sig_5%  \\\n",
       "0    141.857993           -282.142007            565.857993   \n",
       "1    160.794818           -263.205182            584.794818   \n",
       "2    160.026839           -263.973161            584.026839   \n",
       "3    184.380423           -239.619577            608.380423   \n",
       "4    147.362692           -276.637308            571.362692   \n",
       "5    154.236193           -269.763807            578.236193   \n",
       "6    149.297739           -274.702261            573.297739   \n",
       "7    182.900183           -241.099817            606.900183   \n",
       "8    165.991866           -258.008134            589.991866   \n",
       "9    119.126067           -304.873933            543.126067   \n",
       "10   167.888482           -256.111518            591.888482   \n",
       "11   108.451820           -315.548180            532.451820   \n",
       "12   107.227924           -316.772076            531.227924   \n",
       "13   219.955514           -204.044486            643.955514   \n",
       "14   140.571496           -283.428504            564.571496   \n",
       "15   197.189960           -226.810040            621.189960   \n",
       "16    82.459703           -341.540297            506.459703   \n",
       "17   200.316383           -223.683617            624.316383   \n",
       "18   173.343209           -250.656791            597.343209   \n",
       "19   157.294197           -266.705803            581.294197   \n",
       "20    61.785655           -362.214345            485.785655   \n",
       "21    85.382582           -338.617418            509.382582   \n",
       "22    72.339728           -351.660272            496.339728   \n",
       "23   177.277197           -246.722803            601.277197   \n",
       "24   106.096554           -317.903446            530.096554   \n",
       "25   168.356287           -255.643713            592.356287   \n",
       "26   101.896663           -322.103337            525.896663   \n",
       "27   171.587117           -252.412883            595.587117   \n",
       "28    50.268041           -373.731959            474.268041   \n",
       "29   207.381926           -216.618074            631.381926   \n",
       "..          ...                   ...                   ...   \n",
       "103  228.246909                   NaN                   NaN   \n",
       "104   71.429132                   NaN                   NaN   \n",
       "105   90.356813                   NaN                   NaN   \n",
       "106   96.652668                   NaN                   NaN   \n",
       "107  108.689151                   NaN                   NaN   \n",
       "108  173.233004                   NaN                   NaN   \n",
       "109  251.276862                   NaN                   NaN   \n",
       "110  151.998726                   NaN                   NaN   \n",
       "111  125.932603                   NaN                   NaN   \n",
       "112  186.182243                   NaN                   NaN   \n",
       "113  245.275804                   NaN                   NaN   \n",
       "114  190.501091                   NaN                   NaN   \n",
       "115  253.293191                   NaN                   NaN   \n",
       "116  139.774058                   NaN                   NaN   \n",
       "117  190.591313                   NaN                   NaN   \n",
       "118  233.721829                   NaN                   NaN   \n",
       "119  123.721149                   NaN                   NaN   \n",
       "120  163.152534                   NaN                   NaN   \n",
       "121  125.887355                   NaN                   NaN   \n",
       "122   98.870407                   NaN                   NaN   \n",
       "123  265.090764                   NaN                   NaN   \n",
       "124  139.216375                   NaN                   NaN   \n",
       "125  119.234854                   NaN                   NaN   \n",
       "126   97.807567                   NaN                   NaN   \n",
       "127  126.773098                   NaN                   NaN   \n",
       "128  142.244951                   NaN                   NaN   \n",
       "129  177.125739                   NaN                   NaN   \n",
       "130  200.285030                   NaN                   NaN   \n",
       "131  134.966040                   NaN                   NaN   \n",
       "132   48.679824                   NaN                   NaN   \n",
       "\n",
       "     lower_lim_set_sig_20%  upper_lim_set_sig_20%  true_label  \n",
       "0              -226.142007             509.857993       151.0  \n",
       "1              -207.205182             528.794818        75.0  \n",
       "2              -207.973161             528.026839       141.0  \n",
       "3              -183.619577             552.380423         NaN  \n",
       "4              -220.637308             515.362692         NaN  \n",
       "5              -213.763807             522.236193         NaN  \n",
       "6              -218.702261             517.297739       138.0  \n",
       "7              -185.099817             550.900183         NaN  \n",
       "8              -202.008134             533.991866         NaN  \n",
       "9              -248.873933             487.126067         NaN  \n",
       "10             -200.111518             535.888482         NaN  \n",
       "11             -259.548180             476.451820         NaN  \n",
       "12             -260.772076             475.227924         NaN  \n",
       "13             -148.044486             587.955514         NaN  \n",
       "14             -227.428504             508.571496         NaN  \n",
       "15             -170.810040             565.189960         NaN  \n",
       "16             -285.540297             450.459703         NaN  \n",
       "17             -167.683617             568.316383         NaN  \n",
       "18             -194.656791             541.343209         NaN  \n",
       "19             -210.705803             525.294197         NaN  \n",
       "20             -306.214345             429.785655        68.0  \n",
       "21             -282.617418             453.382582         NaN  \n",
       "22             -295.660272             440.339728         NaN  \n",
       "23             -190.722803             545.277197       245.0  \n",
       "24             -261.903446             474.096554       184.0  \n",
       "25             -199.643713             536.356287       202.0  \n",
       "26             -266.103337             469.896663         NaN  \n",
       "27             -196.412883             539.587117         NaN  \n",
       "28             -317.731959             418.268041         NaN  \n",
       "29             -160.618074             575.381926         NaN  \n",
       "..                     ...                    ...         ...  \n",
       "103                    NaN                    NaN         NaN  \n",
       "104                    NaN                    NaN         NaN  \n",
       "105                    NaN                    NaN         NaN  \n",
       "106                    NaN                    NaN         NaN  \n",
       "107                    NaN                    NaN         NaN  \n",
       "108                    NaN                    NaN         NaN  \n",
       "109                    NaN                    NaN         NaN  \n",
       "110                    NaN                    NaN         NaN  \n",
       "111                    NaN                    NaN         NaN  \n",
       "112                    NaN                    NaN         NaN  \n",
       "113                    NaN                    NaN         NaN  \n",
       "114                    NaN                    NaN         NaN  \n",
       "115                    NaN                    NaN         NaN  \n",
       "116                    NaN                    NaN         NaN  \n",
       "117                    NaN                    NaN         NaN  \n",
       "118                    NaN                    NaN         NaN  \n",
       "119                    NaN                    NaN         NaN  \n",
       "120                    NaN                    NaN         NaN  \n",
       "121                    NaN                    NaN         NaN  \n",
       "122                    NaN                    NaN         NaN  \n",
       "123                    NaN                    NaN         NaN  \n",
       "124                    NaN                    NaN         NaN  \n",
       "125                    NaN                    NaN         NaN  \n",
       "126                    NaN                    NaN         NaN  \n",
       "127                    NaN                    NaN         NaN  \n",
       "128                    NaN                    NaN         NaN  \n",
       "129                    NaN                    NaN         NaN  \n",
       "130                    NaN                    NaN         NaN  \n",
       "131                    NaN                    NaN         NaN  \n",
       "132                    NaN                    NaN         NaN  \n",
       "\n",
       "[133 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_2\n",
    "# 0 is predicted label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for significance at 5%\n",
      "0.0\n",
      "Error rate for significance at 20%\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Error rate for significance at 5%\")\n",
    "print(test_error_conf_pred_sig5)\n",
    "print(\"Error rate for significance at 20%\")\n",
    "print(test_error_conf_pred_sig20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
